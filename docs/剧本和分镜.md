# Claude Code 写剧本和分镜，Nana Banana Pro生成素材图，Seedance 2.0做视频生成，三件套配合，做了一套20集的水墨国风 AI 漫剧



用 Seedance 2.0 做了 20 集水墨漫剧，聊聊真实体验
最近网上铺天盖地都在说Seedance 2.0强得离谱，各种十几秒的演示视频到处都是。

但说实话看多了我也有疑问，全是挑出来效果最好的那些，真拿它做个完整的东西，到底行不行？

实践出真知，趁春节有空，我决定自己试试。

我用Claude Code 写剧本和分镜，Nana Banana Pro生成素材图，Seedance 2.0做视频生成，三件套配合，做了一套20集的水墨国风 AI 漫剧，取名叫《墨剑吟》。每集 15 秒，加上片头，总共约 5 分多钟。风格是水墨画加动漫渲染，类似《雾山五行》那种质感。

先放成品，大家可以自己感受下：

做完了聊聊真实感受：Seedance 2.0确实很强，但目前还没有网上吹得那么玄乎。下面把整个制作流程拆开聊聊，踩过的坑也一并说了。

整个流程长什么样
制作这套AI漫剧，我总共分了六步：

构思主题 → 写剧本 → 生成素材描述 → 生图 → 写分镜脚本 → 逐集生成视频
Claude Code基本上包办了所有跟文字相关的活儿。构思的时候跟它讨论题材和故事走向，聊完让它直接出剧本。有了剧本之后，再让它输出所有角色、场景、道具的素材描述（英文 prompt）。素材图生成完之后，再让Claude Code结合剧本和即梦Seedance 2.0官方使用手册，输出每一集的分镜脚本。最后还让它做质量检查，审查所有分镜的完整性和首尾连续性。

手动操作集中在两个地方：用 Nana Banana Pro生产素材图，以及在即梦平台上逐集粘贴 prompt 生成视频。

最终产出的数据量：一份完整剧本（四幕二十集）、30 张素材图（13 张角色图 + 12 张场景图 + 5 张道具图）、21 份分镜脚本（含 1 个片头）。

先有个故事
第一步是跟Claude聊主题。我就想做武侠，原因是从小就有武侠梦。跟Claude讨论了几轮之后，确定了故事框架。用的是经典的四幕结构，起承转合，每幕五集。

故事讲的是青年剑客叶青云的师门被灭，他踏上复仇之路，途中救下被追杀的女子苏挽月。两人结伴同行，慢慢有了感情。但苏挽月的真实身份竟是灭门仇人玄冥教主燕无痕的亲妹妹，她偷出了上古剑谱叛逃。叶青云面临复仇与爱情的两难，最终与苏挽月联手击败燕无痕，将剑谱付之一炬，归隐山林。

每集的情绪基调也提前定好了。第一幕从沉重到温暖，第二幕从意外到冲突，第三幕从纠结到激烈，第四幕从对决到放下。每集都有明确的情绪基调，这样后面写分镜的时候，音效和色调都有据可依。

这一步花的时间不多，加上人工检查，大概两个小时。但后面所有东西都跟着剧本走，剧本没弄好，后面全得返工。

图片
30 张参考图，全靠 Nana Banana Pro
有了剧本，下一步是让Claude把所有需要的视觉素材列出来。每个素材都编了号，方便后续在分镜脚本里用@语法引用。

类别
数量
编号范围
示例
角色
13 张
C01-C13
C01 叶青云·正面全身、C04 苏挽月·正面全身、C07 燕无痕·正面全身
场景
12 张
S01-S12
S01 青锋山废墟、S03 月下古镇、S04 悬崖绝壁
道具
5 张
P01-P05
P01 青锋剑、P03 剑谱《墨剑吟》、P04 半面鬼面具
同一个角色可能有多张图。比如叶青云就有正面全身（C01）、侧面动态（C02）、背影远行（C03）、受伤状态（C11）、少年时期（C13）五张。不同集的场景不同，需要引用不同角度和状态的角色图。

30 张图的风格必须统一。要是有的偏写实有的偏卡通，做出来的视频就很违和。我的做法是给所有 prompt 加统一的风格前缀：

Chinese ink wash painting style mixed with anime cel-shading
所有角色、场景、道具图都以这句话开头，保证出来的风格统一。例如：

### C01 — 叶青云·正面全身立绘

Chinese ink wash painting style mixed with anime cel-shading, a young male swordsman standing in a heroic pose, full body front view. He has long black hair tied in a high ponytail, sharp eyebrows, bright determined eyes, handsome angular face with a resolute expression. He wears a blue-white hanfu swordsman robe with flowing sleeves, a dark ink-black waist sash, black wrist guard on left arm. An ancient longsword named "Qingfeng" is strapped to his back in a simple dark scabbard. Age around 22. Athletic lean build. Ink splatter effects on the edges. Muted color palette with blue-white-black tones. Traditional Chinese painting background with subtle mountain silhouettes. Full body character design sheet style, clean background, highly detailed costume design
角色辨识度靠颜色和视觉标记来区分。叶青云是黑色高马尾加青白色长袍加背负长剑，苏挽月是银白色编发加月白长裙加腰间短剑，燕无痕是半面鬼面具加黑袍暗红披甲。这三个人的颜色差异足够大，在水墨风格里也能一眼认出来。

生图全部用Nana Banana Pro完成。生成尺寸选 16:9 横屏，跟视频比例一致。这里有个细节需要注意：Nana Banana Pro生成的图片带水印，一定要去 banana.ovo.re 去除，不然水印可能会被带进视频里，容易穿帮。

Seedance官方实战手册很重要
写分镜之前，我先干了一件事：把即梦官方出的Seedance 2.0使用手册扒了下来。

手册发在飞书上，地址是：https://bytedance.larkoffice.com/wiki/A5RHwWhoBiOnjukIIw6cu5ybnXQ。内容非常详细，涵盖了模型的各种能力和最佳实践。我获取到全部内容后，保存成markdown文件，然后在使用Claude Code 生成分镜脚本时，把这份文件作为上下文一起喂进去。

这样Claude Code写prompt的时候就有参考了，出来的分镜脚本会尽量遵循Seedance 2.0的最佳实践。

这步的收获很大。比如手册里推荐用时间线结构来写 prompt，把 15 秒拆成五个3 秒时段，每段分别描述画面、运镜和音效。这比笼统地说"整段视频要有什么"好使多了，模型对每个时段该干嘛理解得更清楚。

图片
“
补充说明：手册覆盖的能力

手册里比较有用的几个知识点：一是时间线 prompt 的分段写法；二是全能参考模式下 @图片X、@视频X、@音频X 的引用语法；三是视频延长功能的使用方式；四是运镜技巧的中文关键词（希区柯克变焦、一镜到底、360度环绕等）；五是每次最多上传 9 张素材图的数量限制。这些规则直接决定了分镜脚本怎么写，强烈建议在动手前完整读一遍。

分镜脚本才是核心
分镜脚本是整个流程里最花功夫的一步，也是最关键的。每集的分镜由三部分组成：上传素材清单、Seedance Prompt、尾帧素材图。

拿第 1 集"灰烬"举个例子。

素材清单列出了这集需要上传的所有参考图：

素材槽
文件
说明
图片1
C01（叶青云正面）
角色一致性参考
图片2
C03（叶青云背影）
尾帧背影参考
图片3
C09（陈望岳半身）
闪回角色参考
图片4
S01（青锋山废墟）
场景背景参考
图片5
P01（青锋剑）
道具参考
图片6
S07（荒野山道）
尾帧场景参考
Seedance Prompt是最核心的部分。以第 1 集为例，按时间线拆成五段：

水墨国风动漫风格。@图片4 作为场景背景参考，@图片1 为主角——一位青年剑客。

0-3s画面：高空俯拍视角缓慢下降前推（推速约0.5倍），从鸟瞰推至平视全景。
被焚毁的山门废墟全景——黑烟升腾，残垣断壁，碎裂匾额散落，
主角孤独地站在废墟中央。风声呼啸，余烬噼啪声。

3-6s画面：镜头继续缓推（推速约1倍），从全景推至中景再推至面部特写。
主角俯身从废墟中拾起师父的断剑（@图片5），剑身残留血迹。
面部特写——眉头紧皱，眼含泪光与怒火交织。低沉悲鸣弦乐。

6-9s画面：快速闪切转场至闪回，画面泛黄模糊处理。
师父（@图片3 的形象）在烈焰中将少年主角推出山门。
火焰轰鸣，模糊喊声。

9-12s画面：闪回结束。环绕镜头逆时针旋转270度。
主角单膝跪地，将断剑插入地面作为墓碑，低头起誓复仇。风声骤停，寂静无声。

12-15s画面：中景→全景后拉至大远景。主角起身，背上长剑（@图片5），
转身沿荒凉山路（@图片6 的山道）走向远方。
背影参考 @图片2，渐行渐远消失在水墨雾气中。悠远箫声渐起。

色调：灰黑、青蓝为主，全程水墨画质感。
每个时段都写了三层信息：画面内容（谁在做什么）、镜头运动（怎么拍）、音效（配什么声音）。@图片X 的引用也分散在各时段里，告诉模型哪个画面参考哪张素材图。

尾帧描述记录了这集最后一帧的画面内容："叶青云背影走在荒凉山道上，枯草随风，远方隐约可见竹林轮廓，大远景构图。"这段描述是给下一集做衔接用的。

图片
所有 20 集加上片头，总共 21 份分镜脚本，都是 Claude Code 基于剧本和手册上下文生成的。生成完之后，我又让 Claude Code 做了一轮完整性审查，主要检查两个点：

一是每集引用的素材编号是否都存在，有没有写错编号或引用了不存在的图。二是相邻两集的首尾帧画面是否连贯。比如第 1 集结尾是叶青云背影走向远方荒野，第 2 集开头就应该是他在荒野山道上独行，画面要接得上。

Claude Code 确实帮忙发现了几处不连贯的地方。但人工还是要最终过一遍，毕竟有些微妙的逻辑断层 AI 不一定能察觉。

“
补充说明：运镜技巧

分镜脚本里用到了十多种运镜技巧，基本都从手册里学来的。比较常用的有：希区柯克变焦（镜头前推同时焦距拉远，人物大小不变但背景骤然压缩，用来表现内心震动）、一镜到底（不切镜连续跟拍，用在动作戏里增加临场感）、360 度环绕（以角色为中心旋转一圈，用在练剑或战斗场景）、时间冻结环绕（画面定格后镜头绕行展示细节，用在关键对决瞬间）、第一人称主观视角（从角色眼睛看出去的画面，用在坠崖场景里效果很好）。在 prompt 里直接用中文关键词写就行，模型能理解。

逐集生成，用"视频延长"功能串起来
分镜脚本全部搞定，接下来就是一集一集生成视频了。

作流程本身不复杂。进入即梦平台，选 Seedance 2.0 的全能参考模式，按分镜脚本上的素材清单逐一上传参考图，然后把 prompt 粘贴进去，选15秒时长，点生成。

关键问题是怎么让20集首尾接上。我用了视频延长功能来串。

片头和第 1 集正常生成，不需要视频参考。从第 2 集开始，每次除了上传角色和场景参考图之外，还要上传上一集生成的视频文件。prompt 开头加一句"将@视频1延长15s"，模型就会以上一集视频的最后画面作为起点，自然衔接新的内容。

这样形成了一条完整的链：

片头(新生成) → E1(新生成) → E2(延长E1) → E3(延长E2) → ... → E20(延长E19)
每一集都是对前一集的延长，首尾帧之间的过渡是模型自动完成的，不需要手动拼接。

当然也不是每次都接得很顺。比如上一集结尾是夜景，下一集该切到白天了，模型有时候过渡得很生硬。这种情况就在 prompt 里显式写转场指令，比如"画面渐暗过渡，天色从暴雨夜晚转为雨后月夜"。

图片
遇到的一些问题
做下来遇到了几个比较普遍的问题，不限于某一集。

敏感词问题。 这是最让人头疼的坑。剧本里有"江湖人士"这个词——一个再普通不过的武侠用语——结果视频一直生成失败。平台不会告诉你失败原因，也不会高亮是哪个词触发了屏蔽，页面上只显示内容不合规生成失败，你完全不知道问题出在哪。

我最后用二分法排查：把 prompt 砍掉后半段，能生成了，说明问题在后半段。再把后半段砍掉一半试，一步步缩小范围，最终定位到是"江湖人士"这四个字。整个排查过程花了将近一个小时。

这个问题目前没有好的解决方案。如果是高频使用 Seedance 的人，建议自己积累一个敏感词库，在让 Claude 生成分镜脚本时就直接规避。

指令遵循不够稳定。 我的分镜脚本写得比较长，每集 prompt 都有三四百字，五个时段加上运镜描述、音效指令、角色引用，信息密度很高。实际跑下来，模型不一定能把所有指令都执行到位。

比较常见的情况是：某个时段的运镜跟描述不一致，或者某个角色在某几秒里突然消失了。这种问题只能重新生成碰运气。有些集重试了三四次才拿到一个还算满意的版本。但也有一些分镜怎么重试都无法完全符合预期，只能暂时挑一个相对满意的。

AI视频编辑功能不好用。这个是目前 Seedance 2.0 比较大的局限。一集 15 秒的视频里，假设 0-3s 那段画面不对，其余 12 秒都挺好，你很难单独修那 3 秒。只能整个 15 秒重新跑一遍。如果新跑出来的版本其他部分反而变差了，就陷入了拆东墙补西墙的循环。

这在时间和成本上都有不小的浪费。期待后续版本能快速提升相关能力。

由于我不太擅长视频编辑，所有的分镜视频都是AI直出的，后续也没有用视频编辑软件做精修，所以能看出视频中还多细节还很粗糙。

强是真的强，颠覆还太早
做完 20 集，我对 Seedance 2.0 的整体感受总结成一句话：能力确实强，但拿来做工业化项目只能说提效，谈取代还为之尚早。

强在哪？

多模态输入是真好用。给它参考图，画面风格和角色长相就有了底，比纯文字生视频靠谱太多。视频延长功能让 20 集能串起来。水墨风格的画面质感也不错，好几个场景出来的效果比我预期的好。

差在哪？

最大的问题是指令遵循不稳定。prompt 写得越复杂，模型越容易不听话，经常得反复重试。视频编辑能力不好用，不能精确修改某几秒。敏感词机制完全是黑箱，排查起来很费劲。

不过话说回来，AI 视频生成领域的迭代速度很快，说不定再过几个月很多问题都解决了。

但不管工具多强，做下来最花心思的还是前期：想好讲什么故事，把剧本弄扎实，把分镜设计好。这些活没法交给工具。